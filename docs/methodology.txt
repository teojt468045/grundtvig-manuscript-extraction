1. Methodological snippet 

All XML files in the Registranten archive were processed using a custom, rule-based Python workflow developed iteratively during the project. The script parses each XML document, extracts relevant metadata fields, and normalizes them into a structured dataset suitable for quantitative analysis. Dating information is derived by searching prioritized descriptive fields (e.g. Indholdsregest, Ms_forste_linie) for contextual year references within the period 1798–1872; where no contextual year is identified, the script falls back to the first valid year occurring in the field. Page counts are extracted from <Antal_sider> and <Antal_blade> using a conservative parsing strategy that isolates the first numeric token or interprets explicitly marked ranges (e.g. “1–10”). When only leaf counts are provided, these are converted to page counts using a 2:1 rule and flagged accordingly. Additional metadata (including bibliographic numbers and fascicle identifiers) is extracted and mapped to a set of fourteen established archival categories. The resulting dataset is exported to Excel with standardized alignment, enabling further statistical analysis and visualisation.

2. Explanation of prompt contributions

The development of the extraction pipeline was shaped by iterative prompt-driven refinement, in which user prompts functioned as specifications for technical implementation rather than sources of interpretive content. Initial prompts defined the core objectives of the workflow, including the extraction of dating information and physical extent from heterogeneous XML records. Subsequent prompts identified structural and semantic variations in the source material (such as page ranges (“1–10”), embedded markup (e.g. <hi>), or ambiguous numerical fields) which led to the incorporation of more robust parsing rules. Additional prompts clarified fallback logic (e.g. deriving page counts from leaf counts), introduced further metadata requirements (bibliographic numbers and fascicle identifiers), and specified the mapping of fascicle identifiers into fourteen analytical categories. Iterative user feedback also guided improvements to error handling, output formatting, and logging. The resulting workflow thus reflects a Human-in-the-Loop process in which generative AI supported technical problem-solving while all methodological decisions and validations remained the responsibility of the researcher.